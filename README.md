# AAAI2026 Submission 

《Less is More: Clustering with Adaptive Probability for Heterogeneous Federated Learning》

## Abstract

Federated learning (FL) is a distributed machine learning paradigm that facilitates collaborative model training without necessitating the sharing of private data. A practical challenge within FL is statistical heterogeneity, which severely hampers both the training efficiency and the model performance. Although cluster-based techniques have been proposed in previous work to mitigate this effects, the selection and adaptation of clusters have posed additional difficulties impacting overall efficiency, alongside the privacy risks associated with intra-cluster data exchange. To address these issues, this paper introduces a novel method, called CAPFed, which performs clustering by adaptive probability while safely forming clusters. The core idea involves conducting clustering predominantly during the most critical periods, i.e., those with the highest information entropy, while reducing clustering that exerts negligible influence on model optimization. The adaptivity of clustering execution is assessed by evaluating local training performance. Furthermore, CAPFed offers identity anonymization that effectively protects against privacy leakage throughout the clustering process. As CAPFed operates independently of inter-client optimization strategies, it allows seamless integration with existing baselines to improve performance. Experimental results demonstrate that our method yields improvements in ∼ 7× in computational efficiency while achieving optimal model performance.

